{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "#### how text tokenization can be done in a basic way.\n",
    "#### I've used Andrej Karpathy's video(https://www.youtube.com/watch?v=zduSFxRajkE&t=2576s&ab_channel=AndrejKarpathy) to do this excersice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer():\n",
    "\n",
    "    def stats(self, ids):\n",
    "        \n",
    "        counts={}\n",
    "        \n",
    "        for pair in zip(ids,ids[1:]):\n",
    "            counts.setdefault(pair,0) \n",
    "            counts[pair] +=1\n",
    "            maxpair=max(counts, key=counts.get)\n",
    "\n",
    "        return counts,maxpair\n",
    "\n",
    "    def merge(self,ids,maxpair,newtok):\n",
    "        newids=[]\n",
    "        for i,pair in enumerate(zip(ids,ids[1:])):\n",
    "            \n",
    "            if pair==maxpair: #if the pair matches the max pair then we need to append newtoken.\n",
    "                newids.append(newtok)\n",
    "            elif ids[i]==maxpair[1] and ids[i-1]==maxpair[0]:  #if the previous cycle was merging cycle, we need to jump one cycle\n",
    "                pass\n",
    "            elif i==len(ids)-2: #if we are at the last pair then we need to append last digit as an extra.\n",
    "                newids.append(ids[i])\n",
    "                newids.append(ids[i+1])\n",
    "            else:\n",
    "                newids.append(ids[i])\n",
    "\n",
    "        return newids\n",
    "\n",
    "\n",
    "    def train(self,text,vocabsize, verbose=False):\n",
    "        assert vocabsize>=256\n",
    "        nummerges = vocabsize-256\n",
    "        ids=list(text.encode(\"utf-8\")) #identifiers are actually bytes forms of the characters in base 10\n",
    "        savedmerges={}\n",
    "        vocab={token:bytes([token]) for token in range(256)}\n",
    "        \n",
    "        for i in range(nummerges):\n",
    "            counts,maxpair=self.stats(ids)\n",
    "            newtok=256+i #the new token will be iterated through number of merges\n",
    "            ids=self.merge(ids,maxpair,newtok)\n",
    "\n",
    "            savedmerges[maxpair]=newtok\n",
    "            vocab[newtok]=vocab[maxpair[0]]+vocab[maxpair[1]]\n",
    "\n",
    "        self.merges=savedmerges\n",
    "        self.tokens=vocab\n",
    "        return savedmerges\n",
    "        \n",
    "\n",
    "    \n",
    "    def encode(self,text):  #this text is a new text. Not the text in the training session.\n",
    "\n",
    "        ids=list(text.encode(\"utf-8\"))\n",
    "        while len(ids)>=2:\n",
    "            textpairs,b=self.stats(ids)\n",
    "            pair=min(textpairs, key=lambda p: self.merges.get(p, float(\"inf\"))) \n",
    "            #minimum newtoken(256 and so on)'s parent pair is needed to be find first.\n",
    "            #Because remember our merging dict is hierarchical. \n",
    "            #And the later merged pairs can be a result of previous merged pairs\n",
    "\n",
    "            if pair not in self.merges: #we still need to check out whether pair in the merges or not\n",
    "                break\n",
    "\n",
    "            newtok=self.merges[pair]\n",
    "            ids=self.merge(ids, pair, newtok)\n",
    "        return ids\n",
    "\n",
    "    def decode(self,ids):\n",
    "        text=b\"\".join(self.vocab[i] for i in ids).decode(\"utf-8\", errors=\"replace\")\n",
    "        return text \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytok=MyTokenizer()\n",
    "merges=mytok.train(text,275,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=mytok.encode(text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
